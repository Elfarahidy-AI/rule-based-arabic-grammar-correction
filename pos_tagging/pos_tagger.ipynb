{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import conllu\n",
    "import nltk\n",
    "from sklearn_crfsuite import CRF\n",
    "from sklearn_crfsuite import metrics\n",
    "from pyarabic.araby import (\n",
    "    tokenize,\n",
    "    is_arabicrange,\n",
    "    strip_tashkeel,\n",
    "    strip_tatweel,\n",
    "    strip_shadda,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "#encode the corpus to numbers\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "import joblib\n",
    "\n",
    "from commons import word_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pos_tagging_dataset.conllu\", mode=\"r\", encoding=\"utf-8\") as data:\n",
    "\n",
    "    #read the file contents and assign under 'annotations'\n",
    "    annotations=data.read()\n",
    "\n",
    "#check the type of the resulting object\n",
    "print(type(annotations))\n",
    "annotations[1:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = conllu.parse(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(sentences))\n",
    "sentences[1].metadata#metadata is simply a dictionary type\n",
    "#meta data has 5 keys:newdoc id ,sent_id,text,original_text,text_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0][0]\n",
    "#each sentence is a row , sentences[sentence_indx][:]\n",
    "# to access each word within the sentence's pos, use sentences[sentence_indx][word_indx]['upos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove diacritics from arabic text and store the new data inside a new file\n",
    "def remove_tashkeel(data):\n",
    "    data_without_diactrics = strip_tashkeel(data)\n",
    "    data_without_shadda = strip_shadda(data_without_diactrics)\n",
    "    data_without_tatweel = strip_tatweel(data_without_shadda)\n",
    "    return data_without_tatweel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a corpus : list of dictionaries, each dict is a sentence\n",
    "corpus=[]\n",
    "list_pos=[]\n",
    "list_words=[]\n",
    "pos=[]\n",
    "list_sentences=[]\n",
    "for sentence in sentences:\n",
    "    word_dict={}\n",
    "    sentence_txt=sentence.metadata['text']\n",
    "    re_pattern = re.compile(r'[a-zA-Z]+')\n",
    "    if re_pattern.search(sentence_txt)== None:\n",
    "        list_sentences.append(sentence_txt)\n",
    "        pos_bfr=[]\n",
    "        for w_indx in range(len(sentence)):\n",
    "            word_str=remove_tashkeel(sentence[w_indx]['form'])\n",
    "            word_pos=sentence[w_indx]['upos']\n",
    "            pos_bfr.append(word_pos)\n",
    "            list_pos.append(word_pos)\n",
    "            list_words.append(word_str)\n",
    "            word_dict[w_indx]=[word_str,word_pos,sentence_txt,len(sentence_txt)]\n",
    "        pos.append(pos_bfr)\n",
    "        corpus.append(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#report the info of the cleaned corpus\n",
    "print(\"The corpus has {} sentences\".format(len(corpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results in a dataframe \n",
    "df_w = pd.DataFrame (list_words, columns = ['w_name'])\n",
    "df = pd.DataFrame (list_pos, columns = ['tag_name'])\n",
    "df_w.to_csv(\"word_list_cleaned.csv\", encoding=\"utf-8\")\n",
    "df.to_csv(\"tag_list.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the found tags with popularity of each in the corpus\n",
    "fig, ax = plt.subplots()\n",
    "# the size of A4 paper\n",
    "fig.set_size_inches(11.7, 8.27)\n",
    "ax = sns.countplot(x=df['tag_name'], order=df['tag_name'].value_counts(ascending=False).index);\n",
    "\n",
    "abs_values = df['tag_name'].value_counts(ascending=False).values\n",
    "\n",
    "ax.bar_label(container=ax.containers[0], labels=abs_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to your CSV files\n",
    "text_file = \"word_list_cleaned.csv\"\n",
    "pos_tags_file = \"tag_list.csv\"\n",
    "\n",
    "# Load the data from CSV files\n",
    "text_data = pd.read_csv(text_file)\n",
    "pos_tags_data = pd.read_csv(pos_tags_file)\n",
    "\n",
    "# Extract the text and POS tags from the DataFrames\n",
    "texts = text_data[\"w_name\"].tolist()\n",
    "pos_tags = pos_tags_data[\"tag_name\"].tolist()\n",
    "\n",
    "corpus = list(zip(texts, pos_tags))\n",
    "# Extract features for each sentence in the corpus\n",
    "X = []\n",
    "y = []\n",
    "for text, pos_tag in zip(texts, pos_tags):\n",
    "    text = str(text)\n",
    "    pos = str(pos)\n",
    "\n",
    "    words = text.split()\n",
    "    pos_list = pos.split()\n",
    "   \n",
    "    words = text.split()\n",
    "    tags = pos_tag.split()\n",
    "    sentence = list(zip(words, tags))\n",
    "    X_sentence = []\n",
    "    y_sentence = []\n",
    "    for i in range(len(sentence)):\n",
    "        X_sentence.append(word_features(sentence, i))\n",
    "        y_sentence.append(sentence[i][1])\n",
    "    X.append(X_sentence)\n",
    "    y.append(y_sentence)\n",
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "split = int(0.9 * len(X))\n",
    "X_train = X\n",
    "y_train = y\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CRF model on the training data\n",
    "crf = CRF(\n",
    "\talgorithm='lbfgs',\n",
    "\tc1=0.01,\n",
    "\tc2=0.05,\n",
    "\tmax_iterations=100,\n",
    "\tall_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data and evaluate the performance\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "print(metrics.flat_accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"و كان الطالبين يدرسان بجد. كان المعلمان يساعدون الطلاب في الفصل.\"\n",
    "text = \"\"\"\n",
    "كان الولدان يلعب في الحديقة.\n",
    "البنت يقرأ كتابًا تحت الشجرة.\n",
    "الطالب تدرسان معًا في المكتبة.\n",
    "ذهبت الأسرة إلى سوق.\n",
    "أشتريت ثلاث كتب من المكتبة.\n",
    "الكتابة على الطاولة.\n",
    "المعلمين يعلم الطلاب في الصف.\n",
    "كان الرجال يجلسون على الكرسي.\n",
    "ألبنات تلعب في الساحتان المتجاورتان الصغيرتان.\n",
    "ألصديق يذهبان إلى المدرسة كل يوم.\n",
    "نريد أن نذهب الى الحديقة.\n",
    "نحب أن نقرأ الكتب.\n",
    "يجب أن نحترم الآخرين.\n",
    "عليكما أن تدرسان بجد.\n",
    "يمكنكم أن تحققون أحلامكم.\n",
    "لن نستسلم أبدا.\n",
    "علينا أن نتعاون معا.\n",
    "يجب أن نحافظ على البيئة.\n",
    "نود أن نسافر حول العالم.\n",
    "لا تنسون أن تتصلوا بي.\n",
    "لم ينجو المصاب من الحادث.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize the text into a list of words\n",
    "# Tokenize the text into a list of words\n",
    "words = text.split()\n",
    "\n",
    "# Create a list of word-tag pairs for the new text\n",
    "sentence = list(zip(words, ['' for _ in range(len(words))]))\n",
    "\n",
    "# Extract features for the words\n",
    "X_new = [word_features(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "# Use the trained CRF model to predict the tags\n",
    "y_pred = crf.predict([X_new])[0]\n",
    "\n",
    "# Print the predicted tags\n",
    "for word, tag in zip(words, y_pred):\n",
    "    print(f\"{word} - {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model \n",
    "joblib.dump(crf, 'models/crf_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "كان AUX\n",
      "المعلمان ADJ\n",
      "منذ ADP\n",
      "قليل ADJ\n",
      "مندهشون NOUN\n"
     ]
    }
   ],
   "source": [
    "crf = joblib.load('models/crf_model.joblib')   \n",
    "text = \"كان المعلمان منذ قليل مندهشون\"\n",
    "words = text.split()\n",
    "sentence = list(zip(words, ['' for _ in range(len(words))]))\n",
    "X_new = [word_features(sentence, i) for i in range(len(sentence))]\n",
    "y_pred = crf.predict([X_new])[0]\n",
    "for word, tag in zip(words, y_pred):\n",
    "    print(word, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
